# Machine Reading Comprehension
## ——Deep Learning Summer School 2017
### Chence Shi/Yuanzheng Tao/Liangchuan Zou/Zirui Wang

1.[R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf)

2.[Learning Natural Language Inference with LSTM](https://arxiv.org/pdf/1512.08849.pdf)

3.[MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER](https://arxiv.org/pdf/1608.07905.pdf)

4.[Question Answering on the SQuAD Dataset](https://web.stanford.edu/class/cs224n/reports/2761899.pdf)

5.[BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION](https://arxiv.org/pdf/1611.01603.pdf)

6.[NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE*](https://arxiv.org/pdf/1409.0473.pdf)

7.[Pointer Networks*](https://arxiv.org/pdf/1506.03134.pdf)

8.[REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION](https://arxiv.org/pdf/1509.06664.pdf)

9.[Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*](https://arxiv.org/pdf/1406.1078.pdf)

10.[ReasoNet: Learning to Stop Reading in Machine Comprehension](https://arxiv.org/pdf/1609.05284.pdf)

11.[Sequence to Sequence Learning with Neural Networks*](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

12.[End-To-End Memory Networks*](https://arxiv.org/pdf/1503.08895.pdf)
